<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="source: Editing 2110.10790 - Snip Web author: Q. VERA LIAO*, Microsoft Research, Canada
 Excerpt PDF & Notes app for STEM"><title>Human-Centered Explainable AI (XAI) From Algorithms to User Experiencesを読む</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://l7cy.github.io/Lunar-Garden//icon.png><link href=https://l7cy.github.io/Lunar-Garden/styles.f7c44321f20fb2fdbfb5b8fd888c8673.min.css rel=stylesheet><link href=https://l7cy.github.io/Lunar-Garden/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://l7cy.github.io/Lunar-Garden/js/darkmode.b4dbacd40120c9508b0b7656b73affe4.min.js></script>
<script src=https://l7cy.github.io/Lunar-Garden/js/util.6f22941e242efae60fd84e7c32e874fa.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script async src=https://l7cy.github.io/Lunar-Garden/js/popover.4093681526f97c74f2b2c960debcb273.min.js></script>
<script defer src=https://l7cy.github.io/Lunar-Garden/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://l7cy.github.io/Lunar-Garden/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://l7cy.github.io/Lunar-Garden/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://l7cy.github.io/Lunar-Garden/",fetchData=Promise.all([fetch("https://l7cy.github.io/Lunar-Garden/indices/linkIndex.c2ba8924620e73aacadb642fbea0f0d9.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://l7cy.github.io/Lunar-Garden/indices/contentIndex.9c939abdee749e205e3f26b0177ce488.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://l7cy.github.io/Lunar-Garden",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://l7cy.github.io/Lunar-Garden",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/l7cy.github.io\/Lunar-Garden\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://l7cy.github.io/Lunar-Garden/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://l7cy.github.io/Lunar-Garden/>Lunar Garden</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Human-Centered Explainable AI (XAI) From Algorithms to User Experiencesを読む</h1><p class=meta>Last updated
Unknown</p><ul class=tags><li><a href=https://l7cy.github.io/Lunar-Garden/tags/2023/02/12/>2023 02 12</a></li></ul><p>source:
<a href=https://snip.mathpix.com/l7cy.at/notes/5e65ce7b-2da7-43dc-9820-f4429ad1af29/edit rel=noopener>Editing 2110.10790 - Snip Web</a>
author: Q. VERA LIAO*, Microsoft Research, Canada</p><blockquote><a href=#excerpt><h2 id=excerpt><span class=hanchor arialabel=Anchor># </span>Excerpt</h2></a><p>PDF & Notes app for STEM</p></blockquote><hr><a href=#human-centered-explainable-ai-xai-from-algorithms-to-user-experiences><h2 id=human-centered-explainable-ai-xai-from-algorithms-to-user-experiences><span class=hanchor arialabel=Anchor># </span>Human-Centered Explainable AI (XAI): From Algorithms to User Experiences</h2></a><p>Q. VERA LIAO*, Microsoft Research, Canada</p><p>KUSH R. VARSHNEY, IBM Research, United States</p><p>(Book Chapter Draft 4/2022) In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question &ldquo;what are human-centered approaches doing for XAI&rdquo; and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users&rsquo; explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.</p><a href=#1-introduction><h2 id=1-introduction><span class=hanchor arialabel=Anchor># </span>1. INTRODUCTION</h2></a><p>In everyday life, people seek explanations when there is a gap of understanding. Explanations are sought for many goals that this understanding is meant to serve, such as predicting future events, diagnosing problems, resolving cognitive dissonance, assigning blame, and rationalizing one&rsquo;s action. In interactions with computing technologies, an appropriate understanding of how the system works, often referred to as users&rsquo; &ldquo;mental model&rdquo; [80], is the foundation for users to correctly anticipate system behaviors and interact effectively. A user&rsquo;s understanding is constantly being shaped by what they see and experience with the system, and can be refined by being directly explained how the system works.</p><p>With the increasing adoption of AI technologies, especially popular inscrutable, opaque machine learning (ML) models such as neural networks models, understanding becomes increasingly difficult. Meanwhile, the need for stakeholders to understand AI is heightened by the uncertain nature of ML systems and the hazardous consequences they can possibly cause, as AI is now frequently deployed in high-stakes domains such as healthcare, finance, transportation, and even criminal justice. Some are concerned that this challenge of understanding will become the bottleneck for people to trust and adopt AI technologies. Others have warned that a lack of human scrutiny will inevitably lead to failures in usability, reliability, safety, fairness, and other moral crises of AI.</p><p>It is with this overwhelming challenge of modern AI that the term explainable AI (XAI) and related terms such as AI &ldquo;interpretability&rdquo; and &ldquo;transpsrency&rdquo; have made their way into numerous academic works, industry efforts, as well as public policy and regulatory requirements. For example, the European Union General Data Protection Regulation (GDPR) now requires that &ldquo;meaningful information about the logic involved&rdquo; must be provided to people who are affected by automated decision-making systems. However, despite a vast collection of XAI algorithms produced by the AI research community and recent emergence of off-the-shelf toolkits (e.g. [1-4, 13]) for AI developers to incorporate state-of-the-art XAI techniques in their own models, successful examples of XAI are still relatively scarce in real-world AI applications.</p><p>*Reviewed work by the first author was done while working at IBM Research</p><p>Authors&rsquo; addresses: Q. Vera Liao, Microsoft Research, Montreal, Canada,
<a rel=noopener class="internal-link broken" data-src=mailto:veraliao@microsoft.com>veraliao@microsoft.com</a>; Kush R. Varshney, IBM Research, Yorktown Heights, United States,
<a rel=noopener class="internal-link broken" data-src=mailto:krvarshn@us.ibm.com>krvarshn@us.ibm.com</a>. Developing XAI applications is challenging because explainability, or the effectiveness of explanation, is not intrinsic to the model but lies in the perception and reception of the person receiving the explanation. Making a model completely transparent to its nuts and bolts does not guarantee that the person at the receiving end can make sense of all the information, or is not overwhelmed. What makes an explanation good-to provide appropriate information that can be understood and utilized-is contingent on the receiver&rsquo;s current knowledge and their goal for receiving the explanation, among other human factors.</p><p>Therefore, developing XAI applications requires human-centered approaches that center the technical development on people&rsquo;s explainability needs, and define success by human experience, empowerment, and well-being. It also means that XAI presents as much of a design challenge as an algorithmic challenge. Hence there are rich opportunities for HCI researchers and design practitioners to contribute insights, solutions, and methods to make AI more explainable. A research community of human-centered XAI [33, 35, 102] has emerged, which bring in cognitive, sociotechnical, design perspectives, and more. We hope this chapter serves as a call to engage in this interdisciplinary endeavor by presenting a selected overview of recent AI and HCI works on the topic of XAI. While the growing collection of XAI algorithms offers a rich toolbox for researchers and practitioners to build XAI applications, we highlight three ways that human-centered approaches can help navigate, assess and expand this toolbox:</p><ul><li><p>There is no one-fits-all solution in the growing collection of XAI techniques. The technical choices should be driven by users&rsquo; explainability needs, for which HCI and user research can offer methodological tools and insights about the design space (Section 3).</p></li><li><p>Empirical studies with real users can reveal pitfalls of existing XAI methods. To overcome the pitfalls requires both design efforts to fill the gaps and reflectively challenging fundamental assumptions in techno-centric approaches to XAI (Section 4).</p></li><li><p>Theories of human cognition and behaviors can offer conceptual tools to inspire new computational and design frameworks for XAI. However, this is still a nascent area and relevant theories in social science, behavioral science, and information science are yet to be explored (Section 5).</p></li></ul><p>Before getting to these points, we will start with a brief overview of the technical landscape of XAI to ground our discussions (Section 2). For interested readers, we suggest several recent papers that provide deeper technical surveys [6,12,21,46]</p><a href=#2-what-is-explainable-ai-and-what-are-the-techniques><h2 id=2-what-is-explainable-ai-and-what-are-the-techniques><span class=hanchor arialabel=Anchor># </span>2. WHAT IS EXPLAINABLE AI AND WHAT ARE THE TECHNIQUES?</h2></a><p>The definitions of explainability and related terms such as transparency, interpretability, intelligibility, and comprehensibility, are in a bit of flux. Scholars sometimes disagree on their scopes and how these terminologies intersect. However, XAI work often shares a common goal of making AI understandable by people. By adopting this pragmatic, human-centered definition in the chapter, we consider XAI as broadly encompassing all technical means to this end of understanding, including direct interpretability, generating an explanation or justification, providing transparency information, etc. (and avoid the philosophical question &ldquo;what is or is not an explanation&rdquo; altogether [82]). We note a distinction between a narrow scope of XAI focusing on explaining the model processes or internals, versus a broad scope that covers all explanatory information about the model, also including information about the training data, performance, uncertainty, and so on [65, 99]. Since our focus is on XAI applications, we believe a broad view is necessary as users are often interested in a holistic understanding of the system and its behaviors. However, technical challenges are commonly presented in the inscrutability of the model internals, so the XAI techniques we review in this section are within the narrower scope of XAI.</p><p>It is worth mentioning that while the majority of XAI work focuses on ML models, and so does this chapter, there are emerging areas of other types of XAI including explainable planning, multi-agent systems, robotics, etc. In fact, the term XAI was coined half a century ago in the context of expert systems. We are currently in its second wave spurred by the popularity of ML technologies.</p><p>At a high level, XAI techniques fall into two camps [46, 68]: 1) choosing a directly interpretable model, such as simpler models like decision trees, rule-based models, and linear regression; 2) choosing a complex, opaque model (sometimes referred to as &ldquo;black-box model&rdquo;), such as deep neural networks and large tree ensembles, and then using a post-hoc technique to generate explanations. The choice between the two is sometimes discussed under the term &ldquo;performance-interpretability tradeoff&rdquo;, as opaque models tend to perform better in many tasks. However, this tradeoff is not always true. Research has shown that in many contexts, especially with well-structured datasets and meaningful features, directly interpretable models can reach comparable performance to opaque models [93]. Moreover, an active research area of XAI focuses on developing new algorithms that possess both performance advantages and interpretable properties. For example, decision sets [60], generalized linear rule models [104], GA2Ms [20], and CoFrNets [87] are recent algorithms that have more advanced computational properties than simple rule-based or linear models, but the model behaviors are still represented in meaningful rules or coefficients that can be understood relatively easily.</p><p>However, opaque models are often chosen in practice because of their performance advantage for a given dataset, often a lower requirement for human effort (e.g., feature engineering), or the availability of off-the-shelf solutions. In these cases, one will have to use post-hoc XAI techniques to make the models explainable. Based on the purposes of explaining, Guidotti et al. [46] categorize post-hoc XAI techniques into global model explanations on the overall logic of the model, outcome explanations focusing on explaining a particular model output, and counterfactual inspection that supports understanding how the model would behave with an alternative input 1. Within these categories, XAI techniques commonly generate either feature-based explanations to elucidate the model internals, or example-based explanations to support case-based reasoning.</p><p>Note the three categories also apply to directly interpretable models. For example, a shallow decision-tree can be presented directly as a global explanation, highlighted of a particular path to explain a prediction outcome, or traced in alternative paths to perform counterfactual inspection. It is, however, much less straightforward with opaque-box models, which require separate post-hoc techniques, as we will give some examples below.</p><p>Examples of global model explanations. Since it is impossible to understand the complex internals of an opaque model, the goal of a global model explanation is to provide an approximate overview of how the model behaves. This is often done by training a simple directly interpretable model such as a decision tree, rule set, or regression with the same training data, and performing optimization to make the simple model behave more closely to the original model. For example, a technique called distillation changes the learning objective of the interpretable model to match the original model&rsquo;s predictions [98]. Depending on the choice of the approximate model, global explanations can take the form of a decision tree, a set of rules, or feature weights. Without burdening people with the model decision complexities, showing representative examples and their predicted outcomes [83,108], which overall have a good coverage representing the input space (whether constructed or extracted from the training data), can be used as example-based global explanations to shed light on how the model behaves for different kinds of input data.</p><p>1 Note some adopt the categorization of global versus local explanations, and consider both outcome explanations and counterfactual explanations as local (concerned about the model decision process in a local region). Examples of outcome explanations. To explain a prediction outcome made on an instance, a number of algorithms can be used to estimate the importance of each feature of this instance contributing to the prediction. For example, LIME (local interpretable model-agnostic explanations) [89] starts by adding a small amount of noise to the instance to create a set of neighbor instances; it fits a simple linear model on those neighbors that mimics the original model&rsquo;s behavior in the local region. The linear model&rsquo;s weights can then be used as the feature importance to explain the prediction. Another popular algorithm SHAP (Shapley additive explanations) [72] defines feature importance based on Shapley values, inspired by cooperative game theory, to assign credit to each feature. Feature-importance explanations can be shown to users by visualizing the importance, or simply describing the most important features for the prediction. To explain deep neural networks, many other algorithms can be used to identify important parts of input features based on gradient [94], propagation [14], occlusion [64], etc. They are sometimes referred to as saliency methods and, when applied to image data, generate saliency maps. Example-based methods are useful to explain an outcome as well. For example, with some notion of similarity, finding similar instances in the training data with the same predicted outcome can be used to justify the prediction [47, 57].</p><p>Examples of counterfactual inspection. Different from outcome explanations that describe the model&rsquo;s decision process for a given instance, counterfactual explanations-&ldquo;counter to the facts&rdquo;-are sought when people are interested in how the model would behave when the current input changes. In other words, people are interested in the &ldquo;why not a different prediction&rdquo; or &ldquo;how to change to get a different prediction&rdquo; questions rather than a descriptive &ldquo;why&rdquo; question. Such explanations are especially sought when seeking remedy or recourse for a current, often undesirable, outcome, such as ways to improve a patient&rsquo;s predicted high risk of a disease. Several algorithms can be used to generate conterfactual explanations by identifying changes, often with some notion of minimum changes, needed for an instance to receive a different prediction [27, 69]. They are sometimes referred to as contrastive explanations for a counterfactual outcome (differentiated from counterfactual causes). Example-based methods can also be used to generate counterfactual examples-instances with minimum differences from the original one but having a different outcome [78,100]. In other situations, people may want to zoom in on a specific feature and explore how its changes impact the model&rsquo;s prediction, i.e. asking a &ldquo;what if&rdquo; question. For this purpose, feature inspection techniques such as partial dependence plot (PDP) [49] and individual conditional expectation (ICE) can be used [44].</p><p>Most post-hoc techniques make some kind of approximation. Distillation and LIME approximate the complex model&rsquo;s behaviors with a simpler model&rsquo;s. PDP leaves out interactions between features. Example-based methods often explain by samples in the data. There is a long-standing debate regarding the potential risk of using approximate post-hoc techniques to explain instead of a directly interpretable model, as approximations will inevitably leave out some corner cases or even be unfaithful to what the original model computes [93].</p><p>However, in addition to the practical reasons mentioned earlier to opt for opaque-box models, there is a pragmatic argument to be made about the diverse communication devices people use to reach &ldquo;sufficient understanding&rdquo; to achieve a given objective. For example, to make a precise diagnosis of a problem, one may need explanations that describe a causal chain; whereas if one&rsquo;s goal is to predict future events, following approximate rules or case-based reasoning could be sufficient and less cognitively demanding. One can also argue that when the model and the person have different epistemic access, approximations can be seen as a form of translation necessary to bridge the two. There is an emerging area of XAI research on generating human-consumable explanations with the supervision of human explanations [34,52,58], which essentially translates model reasoning into meaningful human explanations applied to the same prediction. This kind of explanation is a complete approximation but could be useful for laypeople who have difficulty understanding how ML models work, but want to get a sense of the validity of its predictions. We further highlight this objective and recipient dependent nature for the choice of explanation methods in the next sections.</p><p>That being said, developers of AI have a responsibility to understand, mitigate, and transparently communicate the limitations of approximate explanations to stakeholders. For example, an explainability metric known as faithfulness can be used to detect faulty post-hoc explanations [7]. We must acknowledge that this is an actively researched topic and there is still a lack of principled approaches to identify and communicate the limitations of post-hoc explanations.</p><a href=#3-diverse-explainability-needs-of-ai-stakeholders><h2 id=3-diverse-explainability-needs-of-ai-stakeholders><span class=hanchor arialabel=Anchor># </span>3. DIVERSE EXPLAINABILITY NEEDS OF AI STAKEHOLDERS</h2></a><p>It is easy to see that there are no &ldquo;one-fits-all&rdquo; solutions from this vast, and still rapidly growing, collection of XAI algorithms, and the choice should be based on target users&rsquo; explainability needs. The challenges are twofold: First, users of XAI are far from a uniform group and their explainability needs can vary significantly depending on their goals, backgrounds, and usage contexts. Second, XAI algorithms were often not developed with specific usage contexts in mind, or were developed primarily to help model developers or AI researchers inspect the models [76]. Hence their appropriateness to support end users&rsquo; explainability needs can be unclear.</p><p>A starting point to address these challenges is to map out the design space of XAI applications and develop frameworks that account for people&rsquo;s diverse explainability needs. At a conceptual level, many attempt to summarize common user groups that demand explainability and what they would use AI explanations for [12, 51, 86]:</p><ul><li><p>Model developers, to improve or debug the model.</p></li><li><p>Business owners or administrators, to assess an AI application&rsquo;s capability, regulatory compliance, etc. to determine its adoption and usage.</p></li><li><p>Decision-makers, who are direct users of AI decision support applications, to form appropriate trust in the AI and make informed decisions.</p></li><li><p>Impacted groups, whose life could be impacted by the AI, to seek recourse or contest the AI.</p></li><li><p>Regulatory bodies, to audit for legal or ethical compliance such as fairness, safety, privacy, etc.</p></li></ul><p>While useful for considering different personas interacting with XAI systems, this kind of categorization lacks granularity to characterize people&rsquo;s explainability needs. For example, a doctor using a patient risk-assessment AI (i.e., a decision-maker) would want to have an overview of the system during the onboarding stage, but delve into AI&rsquo;s reasoning for a particular patient&rsquo;s risk assessment when they treat the patient. Also, people in any of these groups may want to assess model capabilities or biases at certain usage points.</p><p>In a recent HCI paper, Suresh et al. [96] define a stakeholder&rsquo;s knowledge and their objectives as two components that cut across to determine one&rsquo;s explainability needs. The authors characterize stakeholders&rsquo; knowledge by formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, data domain, and general milieu. For stakeholders&rsquo; goals and objectives, the authors propose a multi-level typology, ranging from longterm goals (building trust and understanding the model), immediate objectives (debug and improve the model, ensure regulatory compliance, take actions based on model output, justify actions influenced by a model, understand data usage, learn about a domain, contest model decisions), and specific tasks to perform with the explanations (assess the reliability of a prediction, detect mistakes, understand information used by the model, understand feature influence, understand model strengths and limitations). While these efforts can be seen as top-down approaches to characterize the overall space of users&rsquo; explainability needs, a complementary approach is to follow user-centered design and start with user research to identify application or interaction specific explainability needs. For example, Eiband et al. [37] proposed a participatory design method that starts with analyzing users&rsquo; current mental model and gaps with how the system should be understood, based on an appropriate mental model prescribed by experts, to identify what needs to be explained.</p><p>In our own research with collaborators, we proposed to identify users&rsquo; explainability needs by eliciting user questions to understand the AI [65]. This notion is based on prior HCI work using prototypical questions to represent &ldquo;intelligibility types&rdquo; [67], and social science literature showing that people&rsquo;s explanatory goals can be expressed in different kinds of questions [50]. By interviewing 20 designers, we collected common questions users ask across 16 ML applications and developed an XAI Question Bank, with more than 50 detailed user questions organized in 9 categories:</p><ul><li><p>How (global model-wide): asking about the general logic or process the AI follows to have a global view.</p></li><li><p>Why (a given prediction): asking about the reason behind a specific prediction.</p></li><li><p>Why Not (a different prediction): asking why the prediction is different from an expected or desired outcome.</p></li><li><p>How to be That (a different prediction) 2 : asking about ways to change the instance to get a different prediction.</p></li><li><p>How to Still Be This (the current prediction): asking what change is allowed for the instance to still get the same prediction.</p></li><li><p>What if: asking how the prediction changes if the input changes.</p></li><li><p>Performance: asking about the performance of the AI.</p></li><li><p>Data: asking about the training data.</p></li><li><p>Output: asking what can be expected or done with the AI&rsquo;s output.</p></li></ul><p>These questions once again demonstrate that XAI should be defined broadly, not limited to explaining model internals, as users are also interested in explanatory information about the performance, data, and scope of output, among other dimensions.</p><p>This XAI question bank maps out the space of common explainability needs and can be used as a tool to identify applicable questions in user research. In a follow-up work [66], we propose a question-driven user centered design method that starts with identifying key user questions by user research, then uses these questions to guide the choices of XAI techniques and iterative design. To facilitate this process and foreground users&rsquo; explanability needs, we suggest to reframe the technical space of XAI by the user question that each XAI technique can address. For example, a feature-importance explanation technique can answer the Why question, while a counterfactual explanation can answer the How to be That question. We provide a suggested mapping between the question categories and example XAI techniques, as shown in Table 1, focusing on techniques that are available in current open-source XAI toolkits accessible for practitioners [1-4]. We suggest that UX practitioners and data scientists should engage in collaborative discussions to identify appropriate XAI techniques, using the mapping guidance as a reference. Based on these technical choices and insights from users questions, XAI user experience can be designed and evaluated in an iterative fashion.</p><p>In short, a growing collection of XAI techniques offer a rich toolbox for researchers and practitioners to build XAI applications. Making effective and responsible choices from this toolbox should be guided by users&rsquo; explainability needs. HCI research offers means to understand user needs for specific applications, insights about real-world user needs to</p><p>2 The difference between Why Not and How to Be That can be subtle and context-dependent. Users may ask Why Not when seeing an unexpected prediction and interested in comparing what gets the counterfactual outcome. Users may ask How to Be That when seeking recourse so the explanation should more specifically focus on minimum or actionable changes they can make to the current input.</p><p><img src="https://cdn.mathpix.com/cropped/2023_02_12_db5e4e5bdd8ea8518b1cg-07.jpg?height=1456&width=1550&top_left_y=318&top_left_x=236" width=auto alt></p><p>Table 1. A mapping guidance between categories of user questions in XAI question bank [65] and example XAI methods to answer these questions, with descriptions of their output in &ldquo;Ways to explain&rdquo; column. XAI methods are selected based on what are available in current open-source XAI toolkits [1-4]. The last three rows (in italic) are broader XAI needs not limited to explaining model processes. This mapping guidance can support identifying appropriate XAI techniques based on user questions.</p><p>better frame and organize this toolbox, as well as methodological tools to help navigate the toolbox. In the next section, we discuss how HCI research can also inform the limitations of the current technical XAI toolbox.</p><a href=#4-pitfalls-of-xai-minding-the-gaps-between-algorithmic-explanations-and><h2 id=4-pitfalls-of-xai-minding-the-gaps-between-algorithmic-explanations-and><span class=hanchor arialabel=Anchor># </span>4. PITFALLS OF XAI: MINDING THE GAPS BETWEEN ALGORITHMIC EXPLANATIONS AND</h2></a><p>ACTIONABLE UNDERSTANDING</p><p>With so many XAI algorithms developed, one must ask: do they work? The answer is complicated because of the diverse contexts where XAI is sought for. The answer is also difficult because it requires understanding how people perceive, process, and use AI explanations. HCI research, and more broadly human-subject studies, are key to evaluating XAI in the context of use [30], identifying where it falls short, and informing human-centered solutions. While many studies showed positive results that XAI techniques can improve people&rsquo;s understanding of the model [48, 61, 70, 90], in this section we draw attention to a few pitfalls of XAI based on recent HCI research, and highlight two sources of disconnect leading to the pitfalls.</p><p>The first source of XAI pitfalls is a disconnect between technical XAI approaches and supporting users&rsquo; end goals in usage contexts, as many empirical studies failed to find conclusive evidence that adding XAI components improve performance for realistic AI-assisted user tasks such as judgment and decision making [15, 85, 103, 109] and image and video analysis [38, 41]. This disconnect must be critically examined by taking the position that users&rsquo; goal with XAI is not an understanding defined in a vacuum, but an actionable understanding that is sufficient to serve the objective that they seek explanations for. However, the diverse and dynamic user objectives that we discussed earlier are often not explicitly considered when developing XAI algorithms. Some even criticized the technical XAI field insofar as having an &ldquo;inmates running the asylum&rdquo; problem [75, 76], as AI researchers often develop algorithms based on their own intuition of what constitutes good explanations rather than the needs of intended users.</p><p>This disconnect is reflected in the current practices of how XAI algorithms are evaluated, which can profoundly shape the output of the field. While progress has been made by recognizing the necessity of human evaluation beyond earlier focuses solely on algorithmic properties, a recent study by Buçinca et al. [17] points out that &ldquo;proxy tasks&rdquo; widely used by AI researchers to evaluate their proposed XAI techniques can be misleading. A common example of a proxy task is a &ldquo;simulation test&rdquo;, which asks people to predict the model&rsquo;s output based on the input and the explanation. Such tests, without specifying an end goal of understanding, can fail to predict the effectiveness of XAI techniques in real tasks that people seek explanations for, such as improving decision-making.</p><p>There can be many reasons for this divide between effectiveness in proxy tasks and deployment. As Buçinca et al. [17] point out, performing proxy tasks in a controlled setting could induce a different cognitive process from a realistic setting, such as granting more attention to the explanation. Moreover, the ability to simulate a model prediction may simply not match the need for a user to perform a realistic task. For instance, in the context of decision-making, the key to the success of a human-AI team is appropriate reliance, i.e. knowing when to trust the AI&rsquo;s recommendation and when to be cautious. An actionable understanding for appropriate reliance requires not only knowing how the model makes predictions, but also how to judge if the reasoning is flawed, for which a user may lack either the knowledge or cognitive capacity. Filling this gap of understanding may require a different kind of transparency. For example, recent HCI studies repeatedly find that showing uncertainty information of individual predictions is more effective than local outcome explanations to help people form appropriate reliance on AI [15, 109].</p><p>To close this gap between technical XAI and user experiences requires both studying user interactions with XAI in the contexts of use, and operationalizing human-centered perspectives in XAI algorithms, including developing evaluation methods that better account for the actual user needs in different downstream usage contexts.</p><p>For the first part, recent HCI research provides useful insights for supporting XAI user experiences in some common usage contexts. For model development or debugging, research suggests that users often need a range of explanations</p><table id=tabular><tbody><tr><td></td><td>f1</td><td>accuracy roc_auc precision recall neg_log_loss</td><td></td><td></td><td></td><td></td></tr><tr><td>LGBM_2</td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.922</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.923</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.923</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.926</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.918</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo><mn>2.66</mn></math></mjx-assistive-mml></mjx-container></span></td></tr><tr><td>LogisticRegression_2 <span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.699</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.712</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.712</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.725</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.675</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo><mn>9.95</mn></math></mjx-assistive-mml></mjx-container></span></td><td></td></tr><tr><td>DecisionTre__2</td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.694</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.707</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.706</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.719</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.67</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo><mn>10.1</mn></math></mjx-assistive-mml></mjx-container></span></td></tr><tr><td>RandomForest_2</td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.752</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.755</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.755</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.756</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.747</mn></math></mjx-assistive-mml></mjx-container></span></td><td><span><mjx-container jax=SVG role=presentation><mjx-assistive-mml role=presentation unselectable=on display=inline><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo><mn>8.46</mn></math></mjx-assistive-mml></mjx-container></span></td></tr></tbody></table><p>(a) Screenshot of the Metrics Table showing metrics for four selected models.</p><p><img src="https://cdn.mathpix.com/cropped/2023_02_12_db5e4e5bdd8ea8518b1cg-09.jpg?height=486&width=699&top_left_y=581&top_left_x=239" width=auto alt></p><p>(b) Partial screenshot of the Feature Importance Comparison View showing 4 of 21Fl plots.</p><p><img src="https://cdn.mathpix.com/cropped/2023_02_12_db5e4e5bdd8ea8518b1cg-09.jpg?height=742&width=767&top_left_y=323&top_left_x=988" width=auto alt></p><p>(c) Screenshot of the Probability Scatterplot Matrix displaying pairwise comparisons of 4 models.</p><p>Fig. 1. Model LineUpper, an example XAI tool in Narkar et al. [79] that supports ML developers to compare multiple candidate models by comparing their feature-importance explanations at multiple levels-at a global level, for a region of input space, or a specific instance, by selecting from the right-side Scatterplot Matrix panel.</p><p>for different levels of the model behaviors to perform a comprehensive diagnosis [53, 54, 79]. For example, Figure 1 shows Model LineUpper [79], an XAI tool that we designed with our collaborators to support data scientists to compare multiple models (in the context of choosing from multiple candidate models generated by automated machine learning (AutoML)) by comparing their feature-importance explanations. This comparison can happen at different levels: for a global view, for an input region selected on the Scatterplot Matrix on the right, and down to individual instances. For decision-makers, such as users of an AI system supporting medical diagnosis [19, 65, 107] , they may want to see a high-level global model explanation during the onboarding stage to form an appropriate mental model, but have higher demand for outcome explanations and counterfactual inspection, especially when they get unexpected or suspicious predictions from the model. When it comes to auditing for model fairness, our work with collaborators compared the effectiveness of four types of explanation (shown in Figure 2) and found that contrastive explanations can effectively help people identify concerns of individual fairness, where similar individuals from different protected groups such as races are treated differently by the model [29].</p><p>The second source of XAI pitfalls brought to light by empirical research is a disconnect between assumptions underlying technical approaches to XAI and people&rsquo;s cognitive processes. A pitfall robustly found in recent work is that explanations can lead to unwarranted trust or confidence in the model. In a controlled experiment where an ML model was used to assist participants to predict apartment sales prices, Poursabzi-Sangdeh et al. [85] found that, contrary to the hypothesis, showing people an explainable model with feature importance hindered their ability to detect model mistakes. By conducting a contextual inquiry study with data scientists using popular XAI techniques (e.g. SHAP) during model development, Kaur et al. [56] found that the existence of explanations could mistakenly lead to over-confidence that the</p><p><img src="https://cdn.mathpix.com/cropped/2023_02_12_db5e4e5bdd8ea8518b1cg-10.jpg?height=605&width=1214&top_left_y=318&top_left_x=518" width=auto alt></p><p>Fig. 2. Four types of XAI features compared in Dodge et al. [29] (with minor updates on the names of explanations from the original paper) to support people&rsquo;s fairness judgment of ML models, with a use case of an ML model performing recidivism risk predictions. Contrastive explanation (top left) focuses on how the defendant would need to change to be predicted to have low risk. It is more effective in revealing individual fairness issues of an unfair model-similar individuals from different protected groups are treated differently-than the two global explanations at the bottom. Example based explanation can suggest fairness issues by revealing the ambivalence of the decision (only 60% of a similar profile re-offend).</p><p>model is ready for deployment. In the context of a nutrition recommender, Eiband et al. [36] showed that even placebic explanations, which did not convey useful information, invoked a similar level of trust as real explanations did. In addition, there is the concern of illusory understanding, with which one subjectively over-estimates the understanding they gain from XAI [23]. Explanations can also create information overload and distract people from forming a useful mental model of how a system operates [95].</p><p>These observations highlight the danger of deploying technologies without a clear understanding of how people interact with them. One way to move the field forward is to connect with theories and insights about human behaviors and cognition. For example, dual-process theories [55,84] provide a critical lens to understand how people process XAI. The central thesis of these theories is that people can engage in two different systems to process information and make decisions. System 1 is intuitive thinking, often following mental shortcuts and heuristics; System 2 is analytical thinking, relying on careful reasoning of information and arguments. Because System 2 is slower and more cognitively demanding, people often resort to System 1 thinking, which, when applied inappropriately, can lead to cognitive biases and sub-optimal decisions. Through this theoretical lens, there is an increasing awareness [17,32,81,88,102] that while XAI techniques make an implicit assumption that people can and will attend to every bit of explanations, in reality, people are more likely to engage in System 1 thinking.</p><p>However, it remains an open question of what kind of heuristics can be triggered by XAI with System 1 thinking. It is possible that people superficially associate the ability to provide explanations directly with competence, and therefore form unwarranted trust and confidence. Heuristics are developed through past experiences, and can evolve as people experience new technologies. Nourani et al. [81] demonstrate that when interacting with XAI, people are vulnerable to common cognitive biases such as anchoring bias after observing model behaviors early on. A recent study by Ehsan et al. [32] uncovered diverse heuristics people follow in response to AI explanations, such as associating explanations with affirmation and social presence, and associating a specific presentation of explanation-numerical numbers-with intelligence and algorithmic thinking. Another critical implication of dual-process theories is that people do not equally engage in System 1 or System 2 thinking in all contexts. People are inclined to engage in System 1 thinking when they lack either the ability or motivation (broadly defined) to perform analytical thinking [84]. This difference can lead to another pitfall of XAIpotential inequalities of experience including the risk of mistrust and misuse of AI. For example, a study found that AI novices, compared to experts, not only had less performance gain from XAI but were also more likely to have illusory satisfaction [97]. Other studies suggest that in time and cognitive resource constrained settings, people are less able to process explanations effectively [92, 107]. In our own work with collaborators [40], we showed that adding explanations in an active learning setting (i.e. label instances requested by the model) decreased satisfaction for people who scored low in Need for Cognition, a personality trait reflecting one&rsquo;s general motivation to engage in effortful thinking.</p><p>Research has begun to address this mismatch between people&rsquo;s cognitive processes and assumptions underlying XAI. One way is to provide interventions to nudge people to engage deeper in System 2 thinking. Buçinca et al. [18] introduced cognitive forcing functions as design interventions for that purpose, including asking users to make decisions before seeing the AI&rsquo;s recommendations, slowing down the process, and letting users choose when to see the AI recommendation. In our own work [88], we also saw that increasing the time for users to interact with the ML system mitigated System 1 biases. Another path is to seek technical and design solutions that reduce the cognitive workload imposed by XAI, by reducing the quantity and improving the consumability of information. For example, studies suggest that muti-modalities (text, visual, audio, etc.) can be leveraged to aid attention and understanding of XAI [92, 97]. Progressive disclosure [95], starting with simplified or high-level transparency information and revealing details later or upon user requests, is another effective approach to reduce cognitive workload. Technical approaches that optimize for a balance between explanation accuracy and conciseness have also been explored [5].</p><p>We must note that heuristics are an indispensable part of people&rsquo;s decision-making process. If applied appropriately, they can aid people to make more efficient and optimal decisions. In fact, they may be key to closing the inequality gaps for people without an ideal profile of ability or motivation to process information about AI. For example, we may envision a quality endorsement feature through some authorized third-party inspecting a model with explanations. This could allow laypeople to apply a reliable &ldquo;authority heuristic&rdquo; and defer to the experts&rsquo; judgments. Understanding what heuristics are involved in interactions with XAI and AI in general, and how to leverage reliable heuristics to improve human-AI interaction, are important open questions for the field.</p><p>We close this section with an optimistic note that by centering our analysis on people, on how they interact with and process information about AI, and whether they can achieve their objectives, we can move away from a techno-centric focus on generating algorithmic explanations. We can begin to identify opportunities to improve user experiences in this currently under-developed space between algorithmic explanations and actionable understanding, and appreciate explainable AI as much of a design problem as a technical problem. The design solutions may be concerned with how to communicate algorithmic explanations effectively, such as choosing the right modalities, level of abstraction, workarounds for privacy or security constraints, and so on. They may also come in the form of interventions to influence how people process XAI, such as providing cognitive forcing functions or guidance that help people better assess explanatory information [91]. Furthermore, it is necessary to fill the knowledge or information gaps for users to achieve actionable understanding beyond algorithmic explanations, such as providing necessary domain knowledge (e.g. what a feature means) and general notions of how AI works.</p><p><img src="https://cdn.mathpix.com/cropped/2023_02_12_db5e4e5bdd8ea8518b1cg-12.jpg?height=691&width=1215&top_left_y=316&top_left_x=520" width=auto alt></p><p>Fig. 3. A scenario-based design of Social Transparency in Al systems as used in [31]. It combines technical explanations and " 4 W features" (What, Who, Why. and When) that reflect the historical decision trajectory of other users to provide transparency into the sociotechnical system.</p><p>While we discussed the pitfalls of XAI mostly through a cognitive lens, implicit in supporting actionable understanding is a requirement to approach XAI as a sociotechnical problem [33], especially given that consequential AI systems are often embedded in socio-organizational contexts with their own history, shared knowledge and norms. On the one hand, for XAI technology developers, to understand the &ldquo;who&rdquo; in XAI and articulate their needs and objectives requires situating the &ldquo;who&rdquo; in the sociotechnical context. On the other hand, for XAI users, an actionable understanding is often a socially situated understanding, which enables them to make sense of not only the technical component but also the sociotechnical system as a whole. Motivated by this sociotechnical perspective, with collaborators we proposed the concept of social transparency-making visible the social-organizational factors that govern the use of AI systems [31]. Operationalized in a design framework to present past users&rsquo; interactions and reasoning with the AI (see a design in Figure 3 studied in [31]), we demonstrated that such information could help users make more informed decisions and improve the collective experience with AI as a sociotechnical system.</p><a href=#5-theory-driven-human-compatible-xai><h2 id=5-theory-driven-human-compatible-xai><span class=hanchor arialabel=Anchor># </span>5. THEORY-DRIVEN HUMAN-COMPATIBLE XAI</h2></a><p>Previously we gave an example of using dual-process theories to retrospectively understand how people interact with XAI. In this section we discuss another important human-centered approach to XAI, by performing theoretical analysis of human explanations, as well as broader cognitive and behavioral processes, to inspire new computational and design frameworks to make XAI more human-compatible.</p><p>Such work is best represented by Miller&rsquo;s seminal paper that brings insights from social sciences about fundamental properties of human explanations to the common awareness of the AI community [75]. By surveying a large volume of prior work on how people seek, generate, and evaluate explanations in philosophy, psychology, and cognitive science, Miller summarized four primary properties of human explanations: 1) Explanations are often contrastive, sought in response to some counterfactual cases. This is because a Why question is often triggered by &ldquo;abnormal or unexpected&rdquo; events, asked to understand the cause of an event relative to some other event that did not occur. In other words, the Why question is often an implicit Why not question. 2) Explanations are selected by the explainer, often in a biased manner. When explaining to others, people rarely give an actual or complete cause of an event, but select a small number of causes based on some criteria or heuristics. 3) Explanations are social, as a transfer of knowledge, often part of a conversation or interaction, and presented relative to the explainer&rsquo;s beliefs about the explainee&rsquo;s beliefs. 4) Using probabilities or statistical information to explain is often ineffective and unsatisfying. Explicitly referring to causes is often more effective.</p><p>Published in 2019, in just two years, this work has made a significant impact on the XAI field. For instance, the point about explanations being contrastive has inspired many to work on counterfactual explanations to answer the Why Not and How to be That questions, as we reviewed in Section 2. A recent work by Alvarez-Melis et al. [8] attempts to operationalize contrastiveness and selectiveness by a &ldquo;weight of evidence&rdquo; framework adapted from information theory. This technique allows users to flexibly inspect current prediction against multiple alternative hypotheses they choose in a compositional way (e.g., &ldquo;a fever rules out a cold in favor of bronchitis or pneumonia; among these, chills suggest the latter&rdquo;), which is especially suitable for multi-class settings.</p><p>From a user-interaction point of view, the points of explanations being selected and social have profound implications. Miller reviewed several useful theories about how people present explanations to others, which we believe can provide conceptual grounds to frame XAI as an interaction problem. One of them is Malle&rsquo;s theory of explanation [74], which breaks the generation of explanations into two distinct and co-influencing psychological processes: 1) Information process for the explainer (i.e. AI in the case of XAI) to devise explanations, which is determined by what kind of information the explainer has access to. 2) Impression management process that governs the social interactions with the explainee (i.e., users in the case of XAI), which is driven by the pragmatic goal of the explainer, such as transferring knowledge, generating trust in the explainee, assigning blame, and so on.</p><p>With this conceptualization, current technical XAI approaches are primarily concerned with the information process, leaving the impression management process an open area yet to be explored, which we believe is key to making XAI effectively selected, less cognitively demanding, and more consumable in general. A useful set of resources to inform XAI work on this topic, as Miller suggested, is to look into the processes of how people select explanations from available causes, by following common heuristics such as abnormality (selecting the abnormal cause), intentionality (select intentional actions), necessity, sufficiency, and robustness (selecting causes that would hold in many situations). The choice highly depends on the explainer&rsquo;s goal, which again highlights the importance of specifying the objective of explaining. Further, we point to broader social science research on impression management [43, 62], on influencing others&rsquo; perception by regulating information in social interactions, as well as ethics discussions around it, to draw inspiration from.</p><p>The social nature of explanation also maps to an essential requirement for interactivity in XAI applications [59]. User interactions do not end at receiving an XAI output, but continue until an actionable understanding is achieved. In other words, as users&rsquo; explainability needs are expressed in questions, they will keep asking follow-up questions until satisfied, thus engaging in back-and-forth conversations. Conversational models of explanation, as well as general principles of conversations and communication (e.g., Grice&rsquo;s maxims that a speaker follows to optimize for the desired social goal [45]; theory of grounding in communication [24]), hold promise for informing technologies and design for interactive XAI. Miller reviewed several relevant theories including Hilton&rsquo;s conversational model of explanations [50], which postulates that a good explanation must be relevant to the focus of a question and present a topology of different causal questions. Antaki and Leudar [9] extended this model to a wider class of argumentative dialogue for the common pattern of claim-backing in explanations. Walton [101] further extended this line of work into a formal dialogue model of explanation, including a set of speech act rules. These theories offer appealing grounds to build computational models, and recent XAI has begun to explore dialogue models for interactive XAI [73]. Outside XAI, work on dialogue systems frequently builds on formal models of human conversational and social interactions (e.g. [16]), including systems that generate explanatory dialogues [22].</p><p>Theories can also inform design frameworks that guide researchers and practitioners to investigate the design space and make design choices. For example, Wang et al. [102] performed a comprehensive analysis on the theoretical underpinning of human reasoning to derive a conceptual framework that allows linking XAI methods to users&rsquo; reasoning needs. This framework includes four dimensions that describe a normative view of how people should reason with explanations, including explanation goals, reasoning process, causal explanation type, and elements in rational choice decisions. It also separately describes people&rsquo;s natural decision-making, and the errors and limitations they are subject to, based on dual-process theories. Designers can use the framework to perform a conceptual analysis to understand, e.g., based on user research, users&rsquo; reasoning goals and potential errors, to identify what XAI methods can support their goals, or to investigate gaps in current XAI methods. The authors further provide a mapping between elements under these human-reasoning dimensions and existing XAI approaches, and guidelines on how to use XAI methods to mitigate common cognitive biases.</p><p>While hugely promising, theory-driven XAI is still a nascent area. Many areas of cognitive, social, and behavioral theories are yet to be explored. For example, given that users of XAI are information seekers to achieve actionable understanding, theories in information science such as models of sense-making [26] and information seeking behaviors [106] (how people&rsquo;s information needs drive their behaviors and information use) can offer useful theoretical lenses to formalize and anticipate user behaviors. However, the major challenge lies in how to operationalize theoretical insights and formal behavioral models into computational and design frameworks, which may require, as many have already argued [30,75,99], collaboration across the research disciplines of AI, HCI, social sciences and more.</p><a href=#6-summary><h2 id=6-summary><span class=hanchor arialabel=Anchor># </span>6. SUMMARY</h2></a><p>Explainable AI is one of the fastest-growing areas of AI in several directions: a rapidly expanding collection of techniques, substantial industry efforts to produce open-source XAI toolkits for practitioners to use, and widespread public awareness and interest in the topic. It is also a fast-growing area for human-centered ML, which can be seen in a proliferation of XAI research published in HCI and social science venues in recent years. Adopting human-centered approaches to XAI is inevitable given that explainability is a human-centric property and XAI must be studied as an interaction problem. However, different from some other topics in this book, HCI work on XAI currently resides in, and often needs to challenge, a techno-centric reality given that the technical AI community has made strides already. A research community of human-centered XAI [33,35,102] has emerged. In this chapter we provide a selected overview on works from this emerging community to help researchers and practitioners understand insights, available resources, and open problems in utilizing XAI techniques to build XAI user experiences. We hope this chapter could encourage future research to continue bridging design practices and state-of-the-art XAI techniques, uncovering pitfalls of and challenging algorithmic assumptions, and building human-compatible XAI from theoretical grounds. We also hope these approaches will inspire work to address broader challenges in human-centered ML.</p><a href=#7-acknowledgments><h2 id=7-acknowledgments><span class=hanchor arialabel=Anchor># </span>7. ACKNOWLEDGMENTS</h2></a><p>We thank Upol Ehsan, Tim Miller, Jenn Wortman Vaughan and Editors of the book for their generous feedback. We are also grateful to members of the Human-AI Collaboration group and the Foundations of Trustworthy AI department at IBM Research - Thomas J. Watson Research Center, whose work and conversations with us shaped our thinking.</p><a href=#8-references><h2 id=8-references><span class=hanchor arialabel=Anchor># </span>8. REFERENCES</h2></a><p>[4] 2019. Microsoft InterpretML. hhttps://github.com/interpretml/interpret.</p><p>[5] Ashraf Abdul, Christian von der Weth, Mohan Kankanhalli, and Brian Y Lim. 2020. COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1-14.</p><p>[6] Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI). IEEE Access 6 (2018), 52138-52160.</p><p>[7] David Alvarez-Melis and Tommi S Jaakkola. 2018. Towards robust interpretability with self-explaining neural networks. arXiv preprint</p><p><img src="https://cdn.mathpix.com/cropped/2023_02_12_db5e4e5bdd8ea8518b1cg-15.jpg?height=38&width=242&top_left_y=694&top_left_x=318" width=auto alt></p><p>[8] David Alvarez-Melis, Harmanpreet Kaur, Hal Daumé III, Hanna Wallach, and Jennifer Wortman Vaughan. 2021. From human explanation to model interpretability: A framework based on weight of evidence. In AAAI Conference on Human Computation and Crowdsourcing (HCOMP).</p><p>[9] Charles Antaki and Ivan Leudar. 1992. Explaining in conversation: Towards an argument model. European fournal of Social Psychology 22, 2 (1992), 181−194</p><p>[10] Daniel W Apley and Jingyu Zhu. 2020. Visualizing the effects of predictor variables in black box supervised learning models. fournal of the Royal Statistical Society: Series B (Statistical Methodology) 82, 4 (2020), 1059-1086.</p><p>[11] Matthew Arnold, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilović, Ravi Nair, K Natesan Ramamurthy, Alexandra Olteanu, David Piorkowski, et al. 2019. FactSheets: Increasing trust in AI services through supplier&rsquo;s declarations of conformity. IBM fournal of Research and Development 63, 4/5 (2019), 6-1.</p><p>[12] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion 58 (2020), 82-115.</p><p>[13] Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilovic, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, and Yunfeng Zhang. 2020. AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models. 7. Mach. Learn. Res. 21, 130 (2020), 1-6.</p><p>[14] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one 10, 7 (2015), e0130140.</p><p>[15] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-16.</p><p>[16] Timothy Bickmore and Justine Cassell. 2001. Relational agents: a model and implementation of building user trust. In Proceedings of the SIGCHI conference on Human factors in computing systems. 396-403.</p><p>[17] Zana Buçinca, Phoebe Lin, Krzysztof Z Gajos, and Elena L Glassman. 2020. Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. In Proceedings of the 25th International Conference on Intelligent User Interfaces. 454-464.</p><p>[18] Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1-21.</p><p>[19] Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2019. Hello AI: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 104.</p><p>[20] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of KDD.</p><p>[21] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. 2019. Machine learning interpretability: A survey on methods and metrics. Electronics 8, 8 (2019), 832.</p><p>[22] Alison Cawsey. 1992. Explanation and interaction: the computer generation of explanatory dialogues. MIT press.</p><p>[23] Michael Chromik, Malin Eiband, Felicitas Buchner, Adrian Krüger, and Andreas Butz. 2021. I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI. In 26th International Conference on Intelligent User Interfaces. 307-317.</p><p>[24] Herbert H Clark and Susan E Brennan. 1991. Grounding in communication. (1991).</p><p>[25] Mark Craven and Jude Shavlik. 1995. Extracting tree-structured representations of trained networks. Advances in neural information processing systems 8 (1995), 24-30.</p><p>[26] Brenda Dervin. 1998. Sense-making theory and practice: An overview of user interests in knowledge seeking and use. Fournal of knowledge management (1998).</p><p>[27] Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das. 2018. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. arXiv preprint arXiv:1802.07623 (2018). [28] Amit Dhurandhar, Karthikeyan Shanmugam, Ronny Luss, and Peder A Olsen. 2018. Improving Simple Models with Confidence Profiles. Advances in Neural Information Processing Systems 31 (2018).</p><p>[29] Jonathan Dodge, Q Vera Liao, Yunfeng Zhang, Rachel KE Bellamy, and Casey Dugan. 2019. Explaining models: an empirical study of how explanations impact fairness judgment. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 275−285.</p><p>[30] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 (2017).</p><p>[31] Upol Ehsan, Q Vera Liao, Michael Muller, Mark O Riedl, and Justin D Weisz. 2021. Expanding explainability: Towards social transparency in ai systems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-19.</p><p>[32] Upol Ehsan, Samir Passi, Q Vera Liao, Larry Chan, I Lee, Michael Muller, Mark O Riedl, et al. 2021. The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations. arXiv preprint arXiv:2107.13509 (2021).</p><p>[33] Upol Ehsan and Mark O Riedl. 2020. Human-centered explainable ai: Towards a reflective sociotechnical approach. In International Conference on Human-Computer Interaction. Springer, 449-466.</p><p>[34] Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, and Mark O Riedl. 2019. Automated rationale generation: a technique for explainable AI and its effects on human perceptions. In Proceedings of the 24th International Conference on Intelligent User Interfaces. 263-274.</p><p>[35] Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Martina Mara, Marc Streit, Sandra Wachter, Andreas Riener, and Mark O Riedl. 2021. Operationalizing Human-Centered Perspectives in Explainable AI. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 1−6.</p><p>[36] Malin Eiband, Daniel Buschek, Alexander Kremer, and Heinrich Hussmann. 2019. The impact of placebic explanations on trust in intelligent systems. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. 1-6.</p><p>[37] Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, and Heinrich Hussmann. 2018. Bringing transparency design into practice. In 23rd international conference on intelligent user interfaces. 211-223.</p><p>[38] Mingming Fan, Xianyou Yang, TszTung Yu, Q Vera Liao, and Jian Zhao. 2022. Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization. Proceedings of the ACM on Human-Computer Interaction 6, CSCW1 (2022), 1-32.</p><p>[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé, III, and Kate Crawford. 2021 Datasheets for Datasets. Commun. ACM 64, 12 (2021), 86-92.</p><p>[40] Bhavya Ghai, Q Vera Liao, Yunfeng Zhang, Rachel Bellamy, and Klaus Mueller. 2021. Explainable active learning (xal) toward ai explanations as interfaces for machine teachers. Proceedings of the ACM on Human-Computer Interaction 4, CSCW3 (2021), 1-28.</p><p>[41] Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew L Beam. 2021. The false hope of current approaches to explainable artificial intelligence in</p><p>[42] Soumya Ghosh, Q Vera Liao, Karthikeyan Natesan Ramamurthy, Jiri Navratil, Prasanna Sattigeri, Kush R Varshney, and Yunfeng Zhang. 2021. Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI. arXiv preprint arXiv:2106.01410</p><p>[43] Erving Goffman et al. 1978. The presentation of self in everyday life. Vol. 21. Harmondsworth London.</p><p>[44] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. journal of Computational and Graphical Statistics 24, 1 (2015), 44-65.</p><p>[45] Herbert P Grice. 1975. Logic and conversation. In Speech acts. Brill, 41-58.</p><p>[46] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2019. A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51, 5 (2019), 93.</p><p>[47] Karthik S Gurumoorthy, Amit Dhurandhar, Guillermo Cecchi, and Charu Aggarwal. 2019. Efficient data representation by selecting prototypes with importance weights. In 2019 IEEE International Conference on Data Mining (ICDM). IEEE, 260-269.</p><p>[48] Peter Hase and Mohit Bansal. 2020. Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?. In Proceedings</p><p>[49] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The elements of statistical learnin. Cited on (2009), 33.</p><p>[51] Michael Hind. 2019. Explaining explainable AI. XRDS: Crossroads, The ACM Magazine for Students 25, 3 (2019), 16-19.</p><p>[52] Michael Hind, Dennis Wei, Murray Campbell, Noel CF Codella, Amit Dhurandhar, Aleksandra Mojsilović, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. 2019. TED: Teaching AI to explain its decisions. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. 123−129</p><p>[53] Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven M Drucker. 2019. Gamut: A design probe to understand how data scientists understand machine learning models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1-13. [57] Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. 2016. Examples are not enough, learn to criticize! criticism for interpretability. In Proceedings of NIPS.</p><p>[58] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668−2677.</p><p>[59] Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with predictions: Visual inspection of black-box machine learning models. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM, 5686-5697.</p><p>[60] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable decision sets: A joint framework for description and prediction. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675-1684.</p><p>[61] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. 2017. Interpretable & explorable approximations of black box models. arXiv preprint arXiv:1707.01154 (2017).</p><p>[62] Mark R Leary and Robin M Kowalski. 1990. Impression management: A literature review and two-component model. Psychological bulletin 107, 1 (1990),34</p><p>[63] Jing Lei, Max G&rsquo;Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. Distribution-free predictive inference for regression. 7. Amer. Statist. Assoc. 113, 523 (2018), 1094-1111.</p><p>[64] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220 (2016).</p><p>[65] Q Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: informing design practices for explainable AI user experiences. In Proceedings of the 2020CHI Conference on Human Factors in Computing Systems. 1-15.</p><p>[66] Q Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, and Daby Sow. 2021. Question-Driven Design Process for Explainable AI User Experiences. arXiv preprint arXiv:2104.03483 (2021).</p><p>[67] Brian Y Lim and Anind K Dey. 2009. Assessing demand for intelligibility in context-aware applications. In Proceedings of the 11th international conference on Ubiquitous computing. 195-204.</p><p>[68] Zachary C Lipton. 2018. The mythos of model interpretability. Queue 16, 3 (2018), 31-57.</p><p>[69] Arnaud Van Looveren and Janis Klaise. 2021. Interpretable counterfactual explanations guided by prototypes. In foint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 650-665.</p><p>[70] Ana Lucic, Hinda Haned, and Maarten de Rijke. 2020. Why does my model fail? contrastive local explanations for retail forecasting. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 90-98.</p><p>[71] Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020. From local explanations to global understanding with explainable AI for trees. Nature machine intelligence 2, 1 (2020), 56-67.</p><p>[72] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st international conference on neural information processing systems. 4768-4777.</p><p>[73] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. 2019. A grounded interaction protocol for explainable artificial intelligence. arXiv preprint arXiv:1903.02409 (2019).</p><p>[74] Bertram F Malle. 2006. How the mind explains behavior: Folk explanations, meaning, and social interaction. Mit Press.</p><p>[75] Tim Miller. 2018. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence (2018).</p><p>[76] Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of inmates running the asylum or: How I learnt to stop worrying and love the social and behavioural sciences. arXiv preprint arXiv:1712.00547 (2017).</p><p>[77] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220-229.</p><p>[78] Ramaravind Kommiya Mothilal, Amit Sharma, and Chenhao Tan. 2019. Explaining machine learning classifiers through diverse counterfactual explanations. arXiv preprint arXiv:1905.07697 (2019).</p><p>[79] Shweta Narkar, Yunfeng Zhang, Q Vera Liao, Dakuo Wang, and Justin D Weisz. 2021. Model LineUpper: Supporting Interactive Model Comparison at Multiple Levels for AutoML. In 26th International Conference on Intelligent User Interfaces. 170-174.</p><p>[80] Don Norman. 2013. The design of everyday things: Revised and expanded edition. Basic books.</p><p>[81] Mahsan Nourani, Chiradeep Roy, Jeremy E Block, Donald R Honeycutt, Tahrima Rahman, Eric Ragan, and Vibhav Gogate. 2021. Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems. In 26th International Conference on Intelligent User Interfaces. 340−350.</p><p>[82] Andrés Páez. 2019. The pragmatic turn in explainable artificial intelligence (XAI). Minds and Machines 29, 3 (2019), 441-459.</p><p>[83] Nicolas Papernot and Patrick McDaniel. 2018. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765(2018).</p><p>[84] Richard E Petty and John T Cacioppo. 1986. The elaboration likelihood model of persuasion. In Communication and persuasion. Springer, 1-24.</p><p>[85] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, and Hanna Wallach. 2021. Manipulating and measuring model interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-52.</p><p>[86] Alun Preece, Dan Harborne, Dave Braines, Richard Tomsett, and Supriyo Chakraborty. 2018. Stakeholders in explainable AI. arXiv preprint arXiv:1810.00184(2018). [87] Isha Puri, Amit Dhurandhar, Tejaswini Pedapati, Karthikeyan Shanmugam, Dennis Wei, and Kush R. Varshney. 2021. CoFrNets: Interpretable neural architecture inspired by continued fractions. In Advances in Neural Information Processing Systems.</p><p>[88] Charvi Rastogi, Yunfeng Zhang, Dennis Wei, Kush R. Varshney, Amit Dhurandhar, and Richard Tomsett. 2022. Deciding Fast and Slow: The Role of Cognitive Biases in AI-Assisted Decision-Making. In Proceedings of the ACM Conference on Computer Supported Cooperative Work and Social Computing.</p><p>[89] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of KDD</p><p>[90] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic explanations. In Thirty-Second AAAI Conference on Artificial Intelligence.</p><p>[91] Soo Young Rieh and David R Danielson. 2007. Credibility: A multidisciplinary framework. Annual review of information science and technology 41, 1 (2007), 307-364.</p><p>[92] Justus Robertson, Athanasios Vasileios Kokkinakis, Jonathan Hook, Ben Kirman, Florian Block, Marian F Ursu, Sagarika Patra, Simon Demediuk, Anders Drachen, and Oluseyi Olarewaju. 2021. Wait, But Why?: Assessing Behavior Explanation Strategies for Real-Time Strategy Games. In 26th International Conference on Intelligent User Interfaces. 32-42.</p><p>[93] Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence 1, 5 (2019), 206-215.</p><p>[94] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision. 618-626.</p><p>[95] Aaron Springer and Steve Whittaker. 2019. Progressive disclosure: empirically motivated approaches to designing effective transparency. In Proceedings of the 24th international conference on intelligent user interfaces. 107-120.</p><p>[96] Harini Suresh, Steven R Gomez, Kevin K Nam, and Arvind Satyanarayan. 2021. Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1−16.</p><p>[97] Maxwell Szymanski, Martijn Millecamp, and Katrien Verbert. 2021. Visual, textual or hybrid: the effect of user expertise on different explanations. In 26th International Conference on Intelligent User Interfaces. 109-119.</p><p>[98] Sarah Tan, Rich Caruana, Giles Hooker, Paul Koch, and Albert Gordo. 2018. Learning global additive explanations for neural nets using model distillation. arXiv preprint arXiv:1801.08640 (2018).</p><p>[99] Jennifer Wortman Vaughan and Hanna Wallach. 2020. A human-centered agenda for intelligible machine learning. Machines We Trust: Getting Along with Artificial Intelligence (2020).</p><p>[100] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2017. Counterfactual explanations without opening the black box: Automated decisions and the GDPR.</p><p>[101] Douglas Walton. 2004. A new dialectical theory of explanation. Philosophical Explorations 7, 1 (2004), 71-89.</p><p>[102] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019. Designing Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 601.</p><p>[103] Xinru Wang and Ming Yin. 2021. Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making. In 26th International Conference on Intelligent User Interfaces. 318-328.</p><p>[104] Dennis Wei, Sanjeeb Dash, Tian Gao, and Oktay Gunluk. 2019. Generalized linear rule models. In International Conference on Machine Learning. PMLR, 6687-6696.</p><p>[105] Pengfei Wei, Zhenzhou Lu, and Jingwen Song. 2015. Variable importance analysis: a comprehensive review. Reliability Engineering & System Safety 142 (2015), 399-432.</p><p>[106] Tom D Wilson. 1981. On user studies and information needs. Fournal of documentation (1981).</p><p>[107] Yao Xie, Melody Chen, David Kao, Ge Gao, and Xiang &lsquo;Anthony&rsquo; Chen. 2020. CheXplain: Enabling Physicians to Explore and Understand Data-Driven, AI-Enabled Medical Imaging Analysis. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1-13.</p><p>[108] Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. 2018. Representer point selection for explaining deep neural networks. Advances in neural information processing systems 31 (2018).</p><p>[109] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 295-305.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/Lunar-Garden/%E8%AA%AD%E3%82%82%E3%81%86%E3%81%A8%E6%80%9D%E3%81%A3%E3%81%A6%E3%81%84%E3%82%8B%E8%AB%96%E6%96%87/ data-ctx="Human-Centered Explainable AI (XAI) From Algorithms to User Experiencesを読む" data-src=/%E8%AA%AD%E3%82%82%E3%81%86%E3%81%A8%E6%80%9D%E3%81%A3%E3%81%A6%E3%81%84%E3%82%8B%E8%AB%96%E6%96%87 class=internal-link>読もうと思っている論文</a></li></ul></div><div style=display:none><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://l7cy.github.io/Lunar-Garden/js/graph.2d9e48dbe7ea47c0ef1c58296ce14448.js></script></div></div><div id=contact_buttons><footer><p>Made by L7Cy using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://l7cy.github.io/Lunar-Garden/>Home</a></li><li><a href=https://twitter.com/>Twitter</a></li><li><a href=https://github.com/L7Cy>GitHub</a></li></ul></footer></div></div></body></html>