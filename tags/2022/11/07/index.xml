<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2022/11/07 on</title><link>https://l7cy.github.io/Lunar-Garden/tags/2022/11/07/</link><description>Recent content in 2022/11/07 on</description><generator>Hugo -- gohugo.io</generator><language>ja</language><atom:link href="https://l7cy.github.io/Lunar-Garden/tags/2022/11/07/index.xml" rel="self" type="application/rss+xml"/><item><title>DeepLearningの世界29</title><link>https://l7cy.github.io/Lunar-Garden/DeepLearning%E3%81%AE%E4%B8%96%E7%95%8C29/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://l7cy.github.io/Lunar-Garden/DeepLearning%E3%81%AE%E4%B8%96%E7%95%8C29/</guid><description>忙しい人のためのTransformer 全体像 Transformerは機械翻訳のモデル Multi-Head Attentionが一番重要 注目すべき情報を上手く選べる 並列処理との相性が良い $$\text{Multi-Head}(Q,K,V)=\text{concat}({head}_i)W^o$$ $${head}_i=\text{Attention}(QW^Q_i,KW^K_i,VW^v_i)$$ $$\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{Q^t!K}{\sqrt{d}}\right)V$$ Scaled Dot-Product Attention 入力$q$に対して，$k$達との類似度を計算して，似ているkeyに対応するvalueを重み付けて足す $$\text{Attention}(\vec{q},k,v)=\text{softmax}\left(\frac{\vec{q}^t!</description></item><item><title>residual connection</title><link>https://l7cy.github.io/Lunar-Garden/residual-connection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://l7cy.github.io/Lunar-Garden/residual-connection/</guid><description> 初期表現・勾配を遠くの奥の層までの長期伝搬を可能にする</description></item></channel></rss>