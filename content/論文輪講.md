# 前回の質問に対する回答
title: 論文輪講
## Q・K・Vとは何か
### 前回のスライドの画像
[![Image from Gyazo](https://i.gyazo.com/e4894c8fcc87295fd993ebd16b5e63ed.png)](https://gyazo.com/e4894c8fcc87295fd993ebd16b5e63ed)

<!--
はじめに、前回の発表での質問に回答したいと思います。
まず、この図にあるクエリ、キー、バリューが何かという質問ですが、
-->
---
### Query・Key・Value
![](https://omathin.com/images/Attention.001.png)
InputとMemoryから生成される
#### Query
Inputデータ（入力の時系列）の中で検索したいものを表す
#### Key
検索すべき対象とQueryがどれだけ似ているかを図る
#### Value
Keyに基づいて適切なValueを出力するための要素

<!--
この図は、さっきの図を少し書き換えたものになります
クエリ、キー、バリューは、このインプットとメモリーから生成されるもので、
クエリは、、
キーは、、
バリューは、、
といった役割かあります。
-->
---
### 何をしているのか
- ![例](https://camo.qiitausercontent.com/0b85036234a8aa52351ecd68c9dcefc96274187e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36313037392f61333232316435652d633032322d666234322d353233392d3030653535353361396334612e706e67)
- QueryとKeyの内積をとることで関連度のようなものを計算
	- ベクトルが似ていれば画像にある各単語の重みが大きくなる
- この重みとKeyに対応するValueの行列積をとる
- 最も関連度の高いものだけでなく，他の単語との関係性も考慮することを可能にしている

<!--
では、具体的に何をしているのかというと、
インプットから作られたクエリと、メモリーから作られたキーの内積をとって、関連度のようなものを計算します。
このベクトルどうしが似ていれば、この図にある重みが大きくなります。
そして、この重みとキーに対応するバリューの行列積をとったものを出力としています。
これによって、最も関連度の高いものだけでなく，他の単語との関係性も考慮することを可能にしている
-->
---
### Self-Attention & SourceTarget-Attention
#### Self-Attention
InputとMemoryが同じ
#### SourceTarget-Attention
InputとMemoryが異なる
![](https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FBL9BicN.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=f29ae39d2fd85c218d67a28656bd97c5)

<!--
この図はTransformerのモデルですが、Attentionの前の入力が全部同じものとそうでないものがあるのがわかると思います。
この、インプットとメモリーに同じものを使うAttentionをSelf-Attention、異なるものを使うAttentionをSourceTarget-Attentionと言います。
-->
---
## Table2のTraining Costとは何か
学習時間，使用するGPUの数，GPUの演算能力の推定値から算出した学習コストの推定値を表す
[![Image from Gyazo](https://i.gyazo.com/e6b8ff6dd0bed8833a1c69cfa653e033.png)](https://gyazo.com/e6b8ff6dd0bed8833a1c69cfa653e033)

<!--
次に、結果の表にあったTraining Costが何を意味するのかという質問ですが、
これは、

-->
---
# 発表する論文
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

<!--

-->
---
# 背景
## Transformer
![](https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FBL9BicN.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=f29ae39d2fd85c218d67a28656bd97c5)
- RNNやCNNを使わずAttentionのみを使用したEncoder-Decoderモデル
- 並列処理で計算を高速化
- 精度も改善

<!--

-->
---
# 課題
- Transformerは，自然言語処理の分野ではデファクトスタンダード
- コンピュータビジョンの分野でもTransformerを使った試みが行われてきた
	- CNNとの組み合わせ
	- CNNを完全に置き換え
		- 計算効率の面で最適化できない
- Transformerを活かしきれていない

<!--

-->
---
# 目的
- Transformerを可能な限りそのまま画像認識に適用する

<!--

-->
---
# 方法
[![Image from Gyazo](https://i.gyazo.com/689397b39f7f8d04efad5fd5cc7dc373.png)](https://gyazo.com/689397b39f7f8d04efad5fd5cc7dc373)
画像をパッチに分割し，それを自然言語処理における単語と同じように扱う

<!--

-->
---
## Linear Projection of Flattened Patches
$$\mathrm{z}_0=[\mathrm{x}_\text{class};\mathrm{x}_p^1\mathrm{E};\mathrm{x}_p^2\mathrm{E};\cdots;\mathrm{x}_p^N\mathrm{E}]+\mathrm{E}_{pos},~~~\mathrm{E}\in\mathbb{R}^{(P^{2}\cdot C)\times D},\mathrm{E}_{pos}\in\mathbb{R}^{(N+1)\times D}\tag{1}$$
$\mathrm{x}^{k}_p$: $k$個目のパッチ
$\mathrm{E}$: 埋め込む行列
- 画像をパッチに分けて平坦化してベクトルに埋め込む
- 相対位置を線形変換で埋め込んだものを各パッチに足す

<!--

-->
---
## Transformer Encoder
$$\begin{align*}
{\mathrm{z}^{\prime}}_\ell=\text{MSA}(\text{LN}(\mathrm{z}_{\ell-1}))+\mathrm{z}_{\ell-1},~~~\ell=1\ldots L\tag{2}\\
\mathrm{z}_\ell=\text{MLP}(\text{LN}({\mathrm{z}^{\prime}}_{\ell}))+{\mathrm{z}^{\prime}}_{\ell},~~~\ell=1\ldots L\tag{3}\\
y=\text{LN}(\mathrm{z}^{0}_L)\tag{4}
\end{align*}$$
- TransformerのEncoder部分を少し変えたもの
- 変えている点
	- 先に正規化する
	- 活性化関数にGELUを使う

<!--

-->
---
## Fine-turning and Higher Resolution
- 大規模なデータセットで事前学習
- タスクごとの小規模なデータセットでFine-turning
- Fine-turningでは，事前学習でのデータセットよりも高い解像度のものを使った方がよい[^1]
[^1]: [[1906.06423] Fixing the train-test resolution discrepancy](https://arxiv.org/abs/1906.06423)

<!--

-->
---
# 実験・結果
## データセット
### Pre-training
|                      | クラス数 |  枚数  |
| -------------------- |:--------:|:------:|
| ILSVRC-2012 ImageNet |   1000   | 130万  |
| ImageNet-21k         |  21000   | 1400万 |
| JFT-300M             |  18000   |  3億   |
- ILSVRC-2012 ImageNet
	- クラス数: 1000
	- 枚数: 130万
- ImageNet-21k
	- クラス数: 21000
	- 枚数: 1400万
- JFT-300M
	- クラス数: 18000
	- 枚数: 3億

<!--

-->
---
### Fine-tuning
- ImageNet
- ImageNet-ReaL
- CIFAR-10/100
- Oxford-IIIT Pets
- Oxford Flowers-102
- VTAB

<!--

-->
---
## モデル
[![Image from Gyazo](https://i.gyazo.com/3590ac610df4602ed6455ff4800b719d.png)](https://gyazo.com/3590ac610df4602ed6455ff4800b719d)
表の3種類

<!--

-->
---
## SOTAモデルとの比較
[![Image from Gyazo](https://i.gyazo.com/ecc65d7a82598e09023f47e2648b528f.png)](https://gyazo.com/ecc65d7a82598e09023f47e2648b528f)
- ほぼ全てのタスクでSOTAを更新
- 計算時間も大幅に削減

<!--

-->
---
## 事前学習時のデータセットの規模で比較
[![Image from Gyazo](https://i.gyazo.com/101347ad402596a0707fb079e5675a00.png)](https://gyazo.com/101347ad402596a0707fb079e5675a00)
- より大規模なデータセットでの事前学習が必要

<!--

-->
---
# まとめ
- 画像のパッチをシーケンスとして扱い，Transformerと同じように処理
- 事前学習が手軽でありながら，SOTAに匹敵するかそれ以上の性能を発揮
# 今後の課題
- 他のコンピュータビジョンタスクへの適用
- さらなる拡張
<!--

-->