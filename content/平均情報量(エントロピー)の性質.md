[[自己情報量]]
title: 平均情報量(エントロピー)の性質
[[平均情報量]]
[[エントロピー関数]]
[[シャノンの補助定理]]
#### さらに一般化したエントロピーの性質
n個の要素からなる確率事象系Aのエントロピーの最大値は$\log n$
$$0\leq H(A)\leq\log n$$
エントロピーが最大になるのは$\forall k$ $P(a_k)=\frac{1}{n}$のとき
$H(A)\leq\log n$の証明
[[条件付きエントロピー]]
[[事前確率]]
[[事後確率]]
[[ベイズの定理]]
偶数だとわかった状況下でのエントロピー
$$H(A|偶数)=-\sum_{k=1}^{n}P(a_k|偶数)\log P(a_k|偶数)$$
偶数・奇数のどちらかであるという情報を取得した場合のエントロピー
H(A|偶数)とH(A|奇数)の期待値
$H(A|B)=H(A|偶数)P(偶数)+H(A|奇数)P(奇数)$
[[条件付きエントロピー]]
[[結合エントロピー]]